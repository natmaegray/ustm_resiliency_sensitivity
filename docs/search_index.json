[["index.html", "USTM Resiliency Sensitivity Analysis 1 Questions", " USTM Resiliency Sensitivity Analysis Gregory Macfarlane Brigham Young University gregmacfarlane@byu.edu Natalie Gray Brigham Young University nat.gray2000@gmail.com 2021-12-06 Abstract This is where the abstract should go. 1 Questions There exists uncertainty in travel demand models. This is known by transportation planners but the majority do not use any particular method to quantify it. This uncertainty exists to some extent by the variance among input parameters. A coefficient of variation can be used to approximate the standard deviation of the inputs, which then provides a range of values that are possible for model input (Zhao and Kockelman 2002). A sampling method can then be used to determine the possible combinations of parameter variance. Two popular sampling methods are Monte Carlo simulation and Latin Hypercube sampling. Monte Carlo simulation is capable of providing full variance probability, but requires large computations to be effective on a large scale model (Yang et al. 2013). Latin Hypercube sampling reduces the amount variants needed, but the question arises if the same result be achieved with fewer samples, and how many samples is that? The research questions are therefore: Using a dummy travel demand model, can Latin Hypercube sampling simplify the iterations needed to approximate random sampling methods (e.g., Monte Carlo simulation)? Does this method of sampling have few enough iterations for statewide model application? References "],["methods.html", " 2 Methods", " 2 Methods To examine the effects of parameter input sensitivity, a 25 zone dummy model was created using data from https://github.com/ActivitySim/activitysim. The skims data was read in to get the values between zones for the distance, the single occupancy vehicle AM time, the walk distance, and the AM time for walk to local bus total in vehicle time. Auto time is simplified using the SOV AM time, nonmotorized uses walking distance multiplied by a factor of average walking speed, and transit time uses the walk to local bus time. The activitysim household data is also used and then organized so that it is capped at 4 persons, 3 vehicles, and 2 workers per household. If the value is larger than that capacity is is marked as the high values. Activitysim doesnâ€™t have trip productions so nhts2017 data https://github.com/byu-transpolab/nhts2017 can be used to approximate them. Only weekday trips are used and the household sizes, vehicles, and workers are capped to the same extent as the activitysim households. The data is summarized by household sizes, vehicles and workers, and the weighted mean of each trip purpose is taken. The three trip purposes used are Home Based Work, Home Based Other, and Non-Home Based. The nhts2017 approximated productions are then applied to each household based on size, vehicles, and workers. Next mode choice parameters (constants and coefficients) are generated. The base values were obtained from the USTM Resiliency model. These values are shown in Table 2.1, and Table 2.2. Table 2.1: Mode Choice Coefficients Name HBW HBO NHB CIVTT -0.0450 -0.0350 -0.0400 CCOST -0.0016 -0.0016 -0.0016 CWALK1 -0.0900 -0.0700 -0.0800 AUTOCOST 18.3000 18.3000 18.3000 Table 2.2: Mode Choice Constants Name HBW HBO NHB K_TRN -0.5140 -0.9853 -1.3020 K_NMOT 1.7602 0.5448 -0.5359 A coefficient of variation is used to identify a standard deviation for each parameter. A set coefficient of variation of 0.30 is used for all six parameters. The standard deviation is equal to 0.30 multiplied by the mean, where the mean values are the base scenario parameters. (SD = CV*mean). The random sampling uses the R function of rnorm. rnorm(1, mean_value, 0.30*mean_value). Ten random samples are taken, and the same function format is used for each of the six variables. Latinhypercube sampling uses the lhs package in R. Since this package only chooses variables on a zero to one scale, the values given the use the function qunif(X[,1], mean_value-0.30*mean_value, mean_value+0.30*mean_value) to put the random sampling on the right scale needed for the given parameter. This is then applied for all six parameters for the five samples of latinhypercube sampling. Now the mode choice logsum can be calculated for each of the list types and their iterations. The utility exuations are as follows: drive_utility = (coeff_ivtt*auto)+(coeff_cost*auto_cost*DIST) nonmo_utility = (k_nmot + 20*(coeff_walk1*nonmotor)) trans_utility = k_trn + (coeff_ivtt*transit) These utilities are then exponentiated, added together, and the natural log is taken to get a logsum value for every origin and destination pair. The shapefile for the dummy model is shown in Figure 2.1. Figure 2.1: TAZ Map with ID The initial model was run, the change was calculated on a random selection basis, and then using latin hyoercube sampling, to see if latin hypercube sampling can estimate uncertainty in fewer runs. "],["findings.html", " 3 Findings", " 3 Findings Which describes the results of what you found. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
